{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/cathleen/miniconda2/bin/python\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 3000+ tweets with 454 for Virgin America and 2884 for United. The tweets contain labeled sentiments. The object of this analysis is to use this dataset to generate label prediction that can be used for other future tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>name</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tr_tweet_1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tr_tweet_2</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tr_tweet_3</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tr_tweet_4</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tr_tweet_5</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tr_tweet_6</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>0.6842</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "      <td>2015-02-24 11:14:33 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tr_tweet_7</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.6745</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>cjmcginnis</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "      <td>2015-02-24 11:13:57 -0800</td>\n",
       "      <td>San Francisco CA</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tr_tweet_8</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.6559</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>dhepburn</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@virginamerica Well, I didn't…but NOW I DO! :-D</td>\n",
       "      <td>2015-02-24 11:11:19 -0800</td>\n",
       "      <td>San Diego</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tr_tweet_9</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>YupitsTate</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@VirginAmerica it was amazing, and arrived an ...</td>\n",
       "      <td>2015-02-24 10:53:27 -0800</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Tr_tweet_10</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>HyperCamiLax</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@VirginAmerica I &amp;lt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 10:30:40 -0800</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Tr_tweet_11</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>HyperCamiLax</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@VirginAmerica This is such a great deal! Alre...</td>\n",
       "      <td>p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 10:30:06 -0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Tr_tweet_12</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>sjespers</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@VirginAmerica Thanks!</td>\n",
       "      <td>2015-02-24 10:15:29 -0800</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Tr_tweet_13</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.6842</td>\n",
       "      <td>Late Flight</td>\n",
       "      <td>0.3684</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>smartwatermelon</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@VirginAmerica SFO-PDX schedule is still MIA.</td>\n",
       "      <td>2015-02-24 10:01:50 -0800</td>\n",
       "      <td>palo alto, ca</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Tr_tweet_14</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>ItzBrianHunty</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@VirginAmerica So excited for my first cross c...</td>\n",
       "      <td>2015-02-24 09:42:59 -0800</td>\n",
       "      <td>west covina</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Tr_tweet_15</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>thebrandiray</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I ❤️ flying @VirginAmerica. ☺️👍</td>\n",
       "      <td>2015-02-24 09:15:00 -0800</td>\n",
       "      <td>Somewhere celebrating life.</td>\n",
       "      <td>Atlantic Time (Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Tr_tweet_16</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>JNLpierce</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@VirginAmerica you know what would be amazingl...</td>\n",
       "      <td>2015-02-24 09:04:10 -0800</td>\n",
       "      <td>Boston | Waltham</td>\n",
       "      <td>Quito</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Tr_tweet_17</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.6705</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>0.3614</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>MISSGJ</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@VirginAmerica why are your first fares in May...</td>\n",
       "      <td>2015-02-24 08:55:56 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Tr_tweet_18</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>DT_Les</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@VirginAmerica I love this graphic. http://t.c...</td>\n",
       "      <td>2015-02-24 08:49:01 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Tr_tweet_19</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>ElvinaBeck</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@VirginAmerica I love the hipster innovation. ...</td>\n",
       "      <td>2015-02-24 08:30:15 -0800</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Tr_tweet_20</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>rjlynch21086</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@VirginAmerica will you be making BOS&amp;gt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 08:27:52 -0800</td>\n",
       "      <td>Boston, MA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0    Tr_tweet_1           neutral                        1.0000   \n",
       "1    Tr_tweet_2          positive                        0.3486   \n",
       "2    Tr_tweet_3           neutral                        0.6837   \n",
       "3    Tr_tweet_4          negative                        1.0000   \n",
       "4    Tr_tweet_5          negative                        1.0000   \n",
       "5    Tr_tweet_6          negative                        1.0000   \n",
       "6    Tr_tweet_7          positive                        0.6745   \n",
       "7    Tr_tweet_8          positive                        0.6559   \n",
       "8    Tr_tweet_9          positive                        1.0000   \n",
       "9   Tr_tweet_10          positive                        1.0000   \n",
       "10  Tr_tweet_11          positive                        1.0000   \n",
       "11  Tr_tweet_12          positive                        1.0000   \n",
       "12  Tr_tweet_13          negative                        0.6842   \n",
       "13  Tr_tweet_14          positive                        1.0000   \n",
       "14  Tr_tweet_15          positive                        1.0000   \n",
       "15  Tr_tweet_16          positive                        1.0000   \n",
       "16  Tr_tweet_17          negative                        0.6705   \n",
       "17  Tr_tweet_18          positive                        1.0000   \n",
       "18  Tr_tweet_19          positive                        1.0000   \n",
       "19  Tr_tweet_20           neutral                        1.0000   \n",
       "\n",
       "   negativereason  negativereason_confidence         airline             name  \\\n",
       "0             NaN                        NaN  Virgin America          cairdin   \n",
       "1             NaN                     0.0000  Virgin America         jnardino   \n",
       "2             NaN                        NaN  Virgin America       yvonnalynn   \n",
       "3      Bad Flight                     0.7033  Virgin America         jnardino   \n",
       "4      Can't Tell                     1.0000  Virgin America         jnardino   \n",
       "5      Can't Tell                     0.6842  Virgin America         jnardino   \n",
       "6             NaN                     0.0000  Virgin America       cjmcginnis   \n",
       "7             NaN                        NaN  Virgin America         dhepburn   \n",
       "8             NaN                        NaN  Virgin America       YupitsTate   \n",
       "9             NaN                        NaN  Virgin America     HyperCamiLax   \n",
       "10            NaN                        NaN  Virgin America     HyperCamiLax   \n",
       "11            NaN                        NaN  Virgin America         sjespers   \n",
       "12    Late Flight                     0.3684  Virgin America  smartwatermelon   \n",
       "13            NaN                        NaN  Virgin America    ItzBrianHunty   \n",
       "14            NaN                        NaN  Virgin America     thebrandiray   \n",
       "15            NaN                        NaN  Virgin America        JNLpierce   \n",
       "16     Can't Tell                     0.3614  Virgin America           MISSGJ   \n",
       "17            NaN                        NaN  Virgin America           DT_Les   \n",
       "18            NaN                        NaN  Virgin America       ElvinaBeck   \n",
       "19            NaN                        NaN  Virgin America     rjlynch21086   \n",
       "\n",
       "    retweet_count                                               text  \\\n",
       "0             0.0                @VirginAmerica What @dhepburn said.   \n",
       "1             0.0  @VirginAmerica plus you've added commercials t...   \n",
       "2             0.0  @VirginAmerica I didn't today... Must mean I n...   \n",
       "3             0.0  @VirginAmerica it's really aggressive to blast...   \n",
       "4             0.0  @VirginAmerica and it's a really big bad thing...   \n",
       "5             0.0  @VirginAmerica seriously would pay $30 a fligh...   \n",
       "6             0.0  @VirginAmerica yes, nearly every time I fly VX...   \n",
       "7             0.0    @virginamerica Well, I didn't…but NOW I DO! :-D   \n",
       "8             0.0  @VirginAmerica it was amazing, and arrived an ...   \n",
       "9             0.0                               @VirginAmerica I &lt   \n",
       "10            0.0  @VirginAmerica This is such a great deal! Alre...   \n",
       "11            0.0                             @VirginAmerica Thanks!   \n",
       "12            0.0      @VirginAmerica SFO-PDX schedule is still MIA.   \n",
       "13            0.0  @VirginAmerica So excited for my first cross c...   \n",
       "14            0.0                    I ❤️ flying @VirginAmerica. ☺️👍   \n",
       "15            0.0  @VirginAmerica you know what would be amazingl...   \n",
       "16            0.0  @VirginAmerica why are your first fares in May...   \n",
       "17            0.0  @VirginAmerica I love this graphic. http://t.c...   \n",
       "18            0.0  @VirginAmerica I love the hipster innovation. ...   \n",
       "19            0.0           @VirginAmerica will you be making BOS&gt   \n",
       "\n",
       "                tweet_created                tweet_location  \\\n",
       "0   2015-02-24 11:35:52 -0800                           NaN   \n",
       "1   2015-02-24 11:15:59 -0800                           NaN   \n",
       "2   2015-02-24 11:15:48 -0800                     Lets Play   \n",
       "3   2015-02-24 11:15:36 -0800                           NaN   \n",
       "4   2015-02-24 11:14:45 -0800                           NaN   \n",
       "5   2015-02-24 11:14:33 -0800                           NaN   \n",
       "6   2015-02-24 11:13:57 -0800              San Francisco CA   \n",
       "7   2015-02-24 11:11:19 -0800                     San Diego   \n",
       "8   2015-02-24 10:53:27 -0800                   Los Angeles   \n",
       "9                         NaN     2015-02-24 10:30:40 -0800   \n",
       "10                          p                           NaN   \n",
       "11  2015-02-24 10:15:29 -0800             San Francisco, CA   \n",
       "12  2015-02-24 10:01:50 -0800                 palo alto, ca   \n",
       "13  2015-02-24 09:42:59 -0800                   west covina   \n",
       "14  2015-02-24 09:15:00 -0800  Somewhere celebrating life.    \n",
       "15  2015-02-24 09:04:10 -0800              Boston | Waltham   \n",
       "16  2015-02-24 08:55:56 -0800                           NaN   \n",
       "17  2015-02-24 08:49:01 -0800                           NaN   \n",
       "18  2015-02-24 08:30:15 -0800                   Los Angeles   \n",
       "19                        NaN     2015-02-24 08:27:52 -0800   \n",
       "\n",
       "                 user_timezone  \n",
       "0   Eastern Time (US & Canada)  \n",
       "1   Pacific Time (US & Canada)  \n",
       "2   Central Time (US & Canada)  \n",
       "3   Pacific Time (US & Canada)  \n",
       "4   Pacific Time (US & Canada)  \n",
       "5   Pacific Time (US & Canada)  \n",
       "6   Pacific Time (US & Canada)  \n",
       "7   Pacific Time (US & Canada)  \n",
       "8   Eastern Time (US & Canada)  \n",
       "9                          NYC  \n",
       "10   2015-02-24 10:30:06 -0800  \n",
       "11  Pacific Time (US & Canada)  \n",
       "12  Pacific Time (US & Canada)  \n",
       "13  Pacific Time (US & Canada)  \n",
       "14      Atlantic Time (Canada)  \n",
       "15                       Quito  \n",
       "16                         NaN  \n",
       "17                         NaN  \n",
       "18  Pacific Time (US & Canada)  \n",
       "19                 Boston, MA   "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentdata = pd.read_csv('data/train.csv')\n",
    "sentdata.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airlines airline\n",
      "United            2884\n",
      "Virgin America     454\n",
      "Name: tweet_id, dtype: int64\n",
      "tweet nmber 3339\n",
      "Sentiment Distribution\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summary of data \n",
    "def createSummary(data):\n",
    "    pd.unique(sentdata['airline'])\n",
    "    airlines = data.groupby('airline').count()['tweet_id']\n",
    "    print('airlines %s' % str(airlines))\n",
    "    num_tweets = data.shape[0]\n",
    "    print('tweet nmber %s' % str(num_tweets))\n",
    "\n",
    "    \n",
    "def sentimentDistribution(data):\n",
    "    print('Sentiment Distribution')\n",
    "    sentiment = data.groupby('airline_sentiment').count()['tweet_id'].reset_index()\n",
    "    g = sns.barplot(x = 'airline_sentiment', y = 'tweet_id', data = sentiment)\n",
    "\n",
    "\n",
    "    \n",
    "createSummary(sentdata)\n",
    "sentimentDistribution(sentdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@virginamerica guys messed seating.. reserved seating friends guys gave seat away ... 😡 want free internet\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "@virginamerica guys messed seating reserved seating friends guys gave seat away 😡 want free internet\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus  import stopwords\n",
    "# tfidf = TfidfVectorizer(strip_accents = None, lowercase = True, preprocessor = None)\n",
    "\n",
    "\n",
    "def removeStopWords(sentdata):\n",
    "    # tfidf.fit_transform(['something','is', 'up', 'up']).toarray()\n",
    "    stop=stopwords.words('english')\n",
    "    punctuation = [',','.',';']\n",
    "    print(sentdata['text'][20])\n",
    "    data = sentdata['text']\n",
    "            \n",
    "    for p in punctuation:\n",
    "        data = data.map(lambda x: x.replace('.', ''))\n",
    "    print(stop)\n",
    "    text=[]\n",
    "    data = data.map(lambda x: ' '.join(word for word in \n",
    "                                str(x).lower().split() \n",
    "                                   if word not in set(stop) ))\n",
    "    print(data[20])\n",
    "    return data\n",
    "#     none=sentdata['text'].map(lambda x:text.append(' '.join\n",
    "#            ([word for word in str(x).lower().strip().split() if not word in set(stop)])))\n",
    "\n",
    "sentdata['text'] = removeStopWords(sentdata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFID  Vectors\n",
    "\n",
    "The first model predict sentiment labels (negative, neutral, and positive) on tfidf vectors. \n",
    "\n",
    "With this vector type, it's clear that the best classification is the Logistic Regression.\n",
    "The reason could be that each column of the word feature is linearly separable and there's minimal interaction \n",
    "features, so logistic regression is able to describe features quite well. The test accuracy is ~72%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cathleen/miniconda2/envs/seinfeld/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "# Model using tfid\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "def createTrainTestData(sentdata):\n",
    "    '''\n",
    "        Transform x into features (tfidf), and y into labels, then split both\n",
    "        into training and test data, default to 20% test \n",
    "    '''\n",
    "    x_features = createTfidfVector(sentdata)\n",
    "    y_label = sentdata['airline_sentiment'].map({'neutral':1,'negative':2,'positive':0})\n",
    "    x_train, x_test, y_train, y_test  = train_test_split(x_features, y_label)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "def createTfidfVector(sentdata):\n",
    "    '''\n",
    "        Create a tfidf vector for each tweet, measuring the importance of each word as  vector \n",
    "    '''\n",
    "    tfid=TfidfVectorizer(strip_accents=None,lowercase=False,preprocessor=None)\n",
    "    x_features=tfid.fit_transform(sentdata['text']).toarray()\n",
    "    return x_features\n",
    "\n",
    "\n",
    "def modelData(model, x_train, y_train, x_test, y_test):\n",
    "    model.fit(x_train, y_train)\n",
    "    score = model.score(x_test, y_test)\n",
    "    return score\n",
    "\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = createTrainTestData(sentdata)\n",
    "\n",
    "def iterateModels(x_train, x_test, y_train, y_test):\n",
    "    models = {\n",
    "         'logistic': LogisticRegression(solver = 'lbfgs', multi_class = 'multinomial'),\n",
    "         'lda': LDA(),\n",
    "         'gradient': GradientBoostingClassifier(),\n",
    "        }\n",
    "    res = {}\n",
    "    for m in models:\n",
    "        res[m] =  modelData(models[m], x_train,  y_train, x_test, y_test)\n",
    "    return res\n",
    "\n",
    "\n",
    "res = iterateModels(x_train, x_test, y_train, y_test)\n",
    "# {'logistic': 0.7221556886227545, 'lda': 0.6239520958083832, 'gradient': 0.7137724550898203}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg Embedding\n",
    "\n",
    "The second model predicts sentiment labels (negative, neutral, and positive) on an average sentence embedding. This is equivalent / similar to bag of words. The word embeddings are pretrained, from fastText. \n",
    "\n",
    "While logistic regression performs about the same using the avg embedding features, it improves the LDA model by 10% accuracy and the gradient boost by 3% accuracy. Using the new training embedding, the gradient boost actually outperforms logistic regression. The reason for this may be the ability for the boot model to capture interaction features in the embeddings. \n",
    "\n",
    "It's likely that if we use neural net approach on these embeddings, we may get even better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cathleen/miniconda2/envs/seinfeld/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7269461077844311"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model using average embeddings \n",
    "import fastText as ft\n",
    "import os\n",
    "import pickle as pk\n",
    "\n",
    "\n",
    "class fastDict():\n",
    "\n",
    "    def __init__(self, read_filename):\n",
    "        # [TODO] allow dynamically init\n",
    "        pickle_filename = '~/FastData/wiki.en/wiki.en.pkl'\n",
    "        self.pickle_path = os.path.expanduser(pickle_filename)\n",
    "\n",
    "\n",
    "    def processDict(self):\n",
    "        # method = store or import\n",
    "        # read pickle dictionary\n",
    "        # if method = store, convert fastText data to pickle format first\n",
    "        return self.loadWordDict()\n",
    "\n",
    "\n",
    "    def loadWordDict(self):\n",
    "        pickle_reader = open(self.pickle_path, 'rb')\n",
    "        word_vec = pk.load(pickle_reader)\n",
    "        return word_vec\n",
    "\n",
    "    \n",
    "    \n",
    "def createSentenceAvgEmbedding(sentdata, word_dict):\n",
    "    '''\n",
    "        Iterate through the fasttext dictionary \n",
    "        and find pre-trained embeddings for each word \n",
    "    '''\n",
    "    x_features = []\n",
    "    for i, sent in sentdata.iterrows():\n",
    "        sentence = sent.text.split()\n",
    "        # init with a vector of length 300\n",
    "        # the embedding size of fasttext\n",
    "        sent_embed = [0]*300\n",
    "        word_ct = 0\n",
    "        for word in sentence:\n",
    "            # otherwise try to add word embedding to \n",
    "            # sentence embedding\n",
    "            # and add to word count\n",
    "            try:\n",
    "                sent_embed += word_dict[word]\n",
    "                word_ct += 1\n",
    "            except:\n",
    "                continue\n",
    "        if word_ct>0:\n",
    "            avg_sent_embed = sent_embed / word_ct\n",
    "        else:\n",
    "            avg_sent_embed = np.array(sent_embed )\n",
    "        x_features.append(avg_sent_embed)\n",
    "    return x_features\n",
    "\n",
    "def createFastEmbedding(sentdata):\n",
    "    read_filename = '~/FastData/wiki.en/wiki.en.bin'\n",
    "    fast = fastDict(read_filename)\n",
    "    word_dict = fast.processDict()\n",
    "    return createSentenceAvgEmbedding(sentdata, word_dict)\n",
    "\n",
    "\n",
    "\n",
    "def createTrainTestEmbedding(sentdata):\n",
    "    '''\n",
    "    '''\n",
    "    x_features = createFastEmbedding(sentdata)\n",
    "    y_label = sentdata['airline_sentiment'].map({'neutral':1,'negative':2,'positive':0})\n",
    "    x_train, x_test, y_train, y_test  = train_test_split(x_features, y_label)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "def modelData(model, x_train, y_train, x_test, y_test):\n",
    "    model.fit(x_train, y_train)\n",
    "    score = model.score(x_test, y_test)\n",
    "    return score\n",
    "\n",
    "\n",
    "def iterateModels(x_train, y_train, x_test,  y_test):\n",
    "    models = {\n",
    "         'logistic': LogisticRegression(solver = 'lbfgs', multi_class = 'multinomial'),\n",
    "         'lda': LDA(),\n",
    "         'gradient': GradientBoostingClassifier(),\n",
    "        }\n",
    "    res = {}\n",
    "    for m in models:\n",
    "        res[m] =  modelData(models[m], x_train,  y_train, x_test, y_test)\n",
    "    return res\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = createTrainTestEmbedding(sentdata)\n",
    "logistic = LogisticRegression(solver = 'lbfgs', multi_class = 'multinomial')\n",
    "iterateModels(x_train, y_train, x_test, y_test)\n",
    "# {'logistic': 0.7269461077844311, 'lda': 0.7161676646706587, 'gradient': 0.7449101796407186}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Neural Net: LSTM vs CNN approach\n",
    "\n",
    "The last approach uses the pretrained embedding layers for each word in the sequence, feeding it into  hidden layer for each sequence to generate prediction. \n",
    "The best performance is LSTM with accuracy 77%, an improvement from either of the methods \n",
    "but not significantly better than the gradient boost approach on average embeddings.\n",
    "\n",
    "The code for this approach can be found in the model directory. \n",
    "\n",
    "LSTM results \n",
    "Overall accuracy: 77% \n",
    "\n",
    "| actual sentiment | % correct prediction |\n",
    "|------------------|----------------------|\n",
    "| positive  |    0.562963 |\n",
    "| neutral   |  0.367089 |\n",
    "| negative   |   0.940959 |\n",
    "\n",
    "\n",
    "CNN results\n",
    "Overall accuracy: 75% \n",
    "\n",
    "| actual sentiment | % correct prediction |\n",
    "|------------------|----------------------|\n",
    "| positive  |    0.561538|\n",
    "| neutral   |     0.404372 |\n",
    "| negative   |    0.919540 |\n",
    "\n",
    "\n",
    "The CNN approach, when well-specified, creates somewhat similar outcome. LSTM is still the best performing, but this might change with further parameter tuning. Interestingly, though worse at predicting negative tweets, the CNN model is much better at predicting neutral tweets. \n",
    "\n",
    "Examples of sentences LSTM misclassified neutral sentiment tweets: \n",
    "\"united rebooting chicago dispatch system need pivotalcf as i'm tired of sitting on planes\"\n",
    "Both CNN and LSTM misclassified this as negative, and could be arguably negative \n",
    "The word waiting and luggage may be associated with negative emotions more. \n",
    "\n",
    "\"united it is possible to make a ticket change via twitter traveling internationally and can't make calls thanks in advance\" \n",
    "LSTM believes this is a positive tweet, perhaps due to the polite \"thanks\", but CNN recognize that this is netural \n",
    "\n",
    "\n",
    "Examples of sentences CNN misclassified negative sentiment tweets: \n",
    "\n",
    "\n",
    "\"united i would like to know what is the easiest way to get a refund.\" \n",
    "CNN thought this was a neutral tweet, probably because it seems to be information seeking, i.e. I would like to know.. whereas LSTM may have indexed more on the word refund. \n",
    "\n",
    "\"united uh i booked it through the ua website why the price change\"\n",
    "Both CNN And LSTM thought this tweet was neutral, likely because the sentence is composed of words related to logistics, with the exception of uh. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (seinfeld)",
   "language": "python",
   "name": "seinfeld"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
